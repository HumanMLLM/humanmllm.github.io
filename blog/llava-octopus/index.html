<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding | Human MLLM</title>
<meta name=keywords content><meta name=description content="PAPER
CODE
Checkpoints
Demo
Introduction
We present LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model&rsquo;s performance in multimodal tasks. LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential."><meta name=author content="Human MLLM"><link rel=canonical href=http://30.220.88.158:1313/blog/llava-octopus/><link crossorigin=anonymous href=/assets/css/stylesheet.687a7bdf1e62c1a0328405768b1a35f45c643e1fb80606ee01a132832cb2ceea.css integrity="sha256-aHp73x5iwaAyhAV2ixo19FxkPh+4BgbuAaEygyyyzuo=" rel="preload stylesheet" as=style><link rel=icon href=http://30.220.88.158:1313/favicon.png><link rel=apple-touch-icon href=http://30.220.88.158:1313/favicon.png><link rel=manifest href=http://30.220.88.158:1313/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=http://30.220.88.158:1313/blog/llava-octopus/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding"><meta property="og:description" content="PAPER
CODE
Checkpoints
Demo
Introduction
We present LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model&rsquo;s performance in multimodal tasks. LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential."><meta property="og:type" content="article"><meta property="og:url" content="http://30.220.88.158:1313/blog/llava-octopus/"><meta property="og:image" content="http://30.220.88.158:1313/blog/llava-octopus/https:/intranetproxy.alipay.com/skylark/lark/0/2025/png/155356495/1737694579583-b3bb81f5-0533-4ceb-9e70-cbbda9c1fe43.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-25T00:00:03+08:00"><meta property="article:modified_time" content="2025-01-25T00:00:03+08:00"><meta property="og:site_name" content="Human MLLM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://30.220.88.158:1313/blog/llava-octopus/https:/intranetproxy.alipay.com/skylark/lark/0/2025/png/155356495/1737694579583-b3bb81f5-0533-4ceb-9e70-cbbda9c1fe43.png"><meta name=twitter:title content="LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding"><meta name=twitter:description content="PAPER
CODE
Checkpoints
Demo
Introduction
We present LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model&rsquo;s performance in multimodal tasks. LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://30.220.88.158:1313/blog/"},{"@type":"ListItem","position":2,"name":"LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding","item":"http://30.220.88.158:1313/blog/llava-octopus/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding","name":"LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding","description":"PAPER CODE Checkpoints Demo\nIntroduction We present LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model\u0026rsquo;s performance in multimodal tasks. LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential.\n","keywords":[],"articleBody":"PAPER CODE Checkpoints Demo\nIntroduction We present LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model’s performance in multimodal tasks. LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential.\nWhy LLaVA-Octopus? Due to the varying video understanding scenarios that different MLLMs are designed to address, the projectors tailored for them exhibit distinct forms and characteristics.\nWe have observed that each kind of projetcor demonstrates unique advantages within its specialized domain. As shown in the upper figure, we present three representative video understanding tasks, offering an intuitive illustration of the characteristics of three typical approaches that employ different specifically designed visual projectors. LLaVA-OneVisionuses image-based projector while VideoLLaMa2 and LLaMA-VID use spatial-temporal projector and token-compress projector, respectively. The results indicate that different visual projectors perform well in their appropriate domains while exhibiting poorer performance in other scenarios. Therefore, we present LLaVA-Octopus, a instruction-driven projector fusion paradigm. **LLaVA-Octopus **introduces a projector fusion gate that integrates the strengths of different visual projectors based on user instructions. In summary, LLaVA-Octopus is able to adaptively adjust the feature weights of various visual projectors according to user instructions, thereby capitalizing on the complementary advantages of each projector.\nWhat’s the difference between LLaVA-Octopus and other paradigm? In the classical paradigm, user instructions are fed into the LLM solely as text tokens. While the instruction-involved paradigm facilitates interaction between instructions and visual features, it is constrained by a single projector. Our proposed instruction-driven projector fusion paradigm designs a projector fusion gate, which dynamically adjusts the weights of different types of visual projectors based on user instructions to produce the fused visual tokens.\nPerformance Model Name MSVD ActivityNet VideoChatGPT MVBench EgoSchema MLVU VideoMME GPT4-V - 59.5 4.06 43.5 55.6 - 60.7 LLaMA-Adapter 54.9 34.2 2.7 31.7 - - - Video-LLaMA 65.3 48.3 2.57 34.1 - - - VideoLLaMA2 70.9 50.2 3.13 54.6 51.7 48.5 46.6 LLaMA-VID 69.7 47.4 2.90 - 38.5 33.2 - VideoChat 56.3 26.5 2.31 35.5 - - - VideoChat2 70.0 49.1 3.02 51.1 54.4 47.9 54.6 LLaVA-Octopus 74.3 53.4 3.19 66.9 59.2 57.5 54.7 Citation @article{zhao2025llavaoctopus,\ntitle={LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding}, author={Jiaxing Zhao and Boyuan Sun and Xiang Chen and Xihan Wei and Qibin Hou}, journal={arXiv preprint arXiv:2501.05067}, year={2025} } ","wordCount":"467","inLanguage":"en","image":"http://30.220.88.158:1313/blog/llava-octopus/https:/intranetproxy.alipay.com/skylark/lark/0/2025/png/155356495/1737694579583-b3bb81f5-0533-4ceb-9e70-cbbda9c1fe43.png","datePublished":"2025-01-25T00:00:03+08:00","dateModified":"2025-01-25T00:00:03+08:00","author":{"@type":"Person","name":"Human MLLM"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://30.220.88.158:1313/blog/llava-octopus/"},"publisher":{"@type":"Organization","name":"Human MLLM","logo":{"@type":"ImageObject","url":"http://30.220.88.158:1313/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="HumanMLLM (Alt + H)"></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/search title="SEARCH (Alt + /)" accesskey=/><span>SEARCH&nbsp;<svg width="13" height="13" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg></span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding</h1><div class=post-meta>&lt;span title='2025-01-25 00:00:03 +0800 CST'>January 25, 2025&lt;/span>&amp;nbsp;·&amp;nbsp;3 min&amp;nbsp;·&amp;nbsp;467 words&amp;nbsp;·&amp;nbsp;Human MLLM</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://arxiv.org/abs/2501.05067 class="btn external" target=_blank>PAPER</a>
<a href=https://github.com/Jiaxing-star/LLaVA-Octopus class="btn external" target=_blank>CODE</a>
<a href=https://github.com/Jiaxing-star/LLaVA-Octopus class="btn external" target=_blank>Checkpoints</a>
<a href=https://github.com/Jiaxing-star/LLaVA-Octopus class="btn external" target=_blank>Demo</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>We present <strong>LLaVA-Octopus</strong>, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the <strong>complementary strengths of each projector</strong>. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model&rsquo;s performance in multimodal tasks. LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as <strong>multimodal understanding, visual question answering, and video understanding</strong>, highlighting its broad application potential.</p><p><img loading=lazy src=https://intranetproxy.alipay.com/skylark/lark/0/2025/png/155356495/1737694579583-b3bb81f5-0533-4ceb-9e70-cbbda9c1fe43.png alt></p><h1 id=why-llava-octopus>Why LLaVA-Octopus?<a hidden class=anchor aria-hidden=true href=#why-llava-octopus>#</a></h1><p><img loading=lazy src=https://intranetproxy.alipay.com/skylark/lark/0/2025/png/155356495/1737696348876-e7adceb2-ba8b-4c36-8cf2-de1e57637bb7.png alt></p><p>Due to the varying video understanding scenarios that different MLLMs are designed to address, the projectors tailored for them exhibit distinct forms and characteristics.</p><p>We have observed that each kind of projetcor demonstrates unique advantages within its specialized domain. As shown in the upper figure, we present three representative video understanding tasks, offering an intuitive illustration of the characteristics of three typical approaches that employ different specifically designed visual projectors. LLaVA-OneVisionuses image-based projector while VideoLLaMa2 and LLaMA-VID use spatial-temporal projector and token-compress projector, respectively. The results indicate that different visual projectors perform well in their appropriate domains while exhibiting poorer performance in other scenarios. Therefore, we present <strong>LLaVA-Octopus</strong>, a instruction-driven projector fusion paradigm. **LLaVA-Octopus **introduces a projector fusion gate that integrates the strengths of different visual projectors based on user instructions. In summary, <strong>LLaVA-Octopus</strong> is able to adaptively adjust the feature weights of various visual projectors according to user instructions, thereby capitalizing on the complementary advantages of each projector.</p><h1 id=whats-the-difference-between-llava-octopus-and-other-paradigm>What&rsquo;s the difference between LLaVA-Octopus and other paradigm?<a hidden class=anchor aria-hidden=true href=#whats-the-difference-between-llava-octopus-and-other-paradigm>#</a></h1><h1 id=httpsintranetproxyalipaycomskylarklark02025png1553564951737694507659-1d860e2f-ee62-42f7-b69b-5e03fb0ee6cfpng><img loading=lazy src=https://intranetproxy.alipay.com/skylark/lark/0/2025/png/155356495/1737694507659-1d860e2f-ee62-42f7-b69b-5e03fb0ee6cf.png alt></h1><p>In the classical paradigm, user instructions are fed into the LLM solely as text tokens. While the instruction-involved paradigm facilitates interaction between instructions and visual features, it is constrained by a single projector. Our proposed instruction-driven projector fusion paradigm designs a projector fusion gate, which dynamically adjusts the weights of different types of visual projectors based on user instructions to produce the fused visual tokens.</p><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><table><thead><tr><th>Model Name</th><th>MSVD</th><th>ActivityNet</th><th>VideoChatGPT</th><th>MVBench</th><th>EgoSchema</th><th>MLVU</th><th>VideoMME</th></tr></thead><tbody><tr><td>GPT4-V</td><td>-</td><td>59.5</td><td>4.06</td><td>43.5</td><td>55.6</td><td>-</td><td>60.7</td></tr><tr><td>LLaMA-Adapter</td><td>54.9</td><td>34.2</td><td>2.7</td><td>31.7</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Video-LLaMA</td><td>65.3</td><td>48.3</td><td>2.57</td><td>34.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VideoLLaMA2</td><td>70.9</td><td>50.2</td><td>3.13</td><td>54.6</td><td>51.7</td><td>48.5</td><td>46.6</td></tr><tr><td>LLaMA-VID</td><td>69.7</td><td>47.4</td><td>2.90</td><td>-</td><td>38.5</td><td>33.2</td><td>-</td></tr><tr><td>VideoChat</td><td>56.3</td><td>26.5</td><td>2.31</td><td>35.5</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VideoChat2</td><td>70.0</td><td>49.1</td><td>3.02</td><td>51.1</td><td>54.4</td><td>47.9</td><td>54.6</td></tr><tr><td><strong>LLaVA-Octopus</strong></td><td><strong>74.3</strong></td><td><strong>53.4</strong></td><td><strong>3.19</strong></td><td><strong>66.9</strong></td><td><strong>59.2</strong></td><td><strong>57.5</strong></td><td><strong>54.7</strong></td></tr></tbody></table><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><blockquote><p>@article{zhao2025llavaoctopus,</p><pre><code>  title={LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding}, 
  author={Jiaxing Zhao and Boyuan Sun and Xiang Chen and Xihan Wei and Qibin Hou},
  journal={arXiv preprint arXiv:2501.05067},
	year={2025}
  }
</code></pre></blockquote></div></article></main><footer class=footer><span>&copy; 2025 <a href=http://30.220.88.158:1313/>Human MLLM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>